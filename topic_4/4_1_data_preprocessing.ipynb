{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION TECHNIQUES\n",
    "## Part I. Before models: data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries and Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read many files at once, I would prefer Spark for it. We need `Spark environment` to run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.driver.memory', '12G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing: articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = '../topic_2/articles_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mask = f'{files_path}/*.json'\n",
    "sdf = spark.read.json(files_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('file', F.input_file_name())\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\n",
    "    'file', \n",
    "    F.regexp_replace(\n",
    "        'file', \n",
    "        'file:///home/jovyan/apbdid_23/topic_2/articles_data/articles_lbl_', \n",
    "        '')\n",
    ")\n",
    "sdf = sdf.withColumn(\n",
    "    'label', \n",
    "    F.regexp_replace(\n",
    "        'file', \n",
    "        '.json', \n",
    "        '')\n",
    ")\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    sdf.label,\n",
    "    F.explode(sdf.articles)\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_df(df, prefix=None):\n",
    "    \"\"\"\n",
    "    Makes nested Spark dataframe flat.\n",
    "    `prefix` is for naming columns\n",
    "    that are uppacked.\n",
    "    \"\"\"\n",
    "    flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\n",
    "    nested_cols = [c[0] for c in df.dtypes if c[1][:6] == 'struct']\n",
    "    flat_df = df.select(\n",
    "        flat_cols + \n",
    "        [F.col(ncol + '.' + col).alias(prefix + col if prefix else ncol + '_' + col ) \n",
    "         for ncol in nested_cols \n",
    "         for col in df.select(ncol + '.*').columns]\n",
    "    )\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flat_df.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds = sdf.select(\n",
    "    sdf.label,\n",
    "    sdf.col_name.alias('title'),\n",
    "    sdf.col_annotation.alias('annotation')\n",
    ")\n",
    "sdf_ds.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds = sdf_ds.withColumn(\n",
    "    'title', \n",
    "    F.regexp_replace(\n",
    "        'title', \n",
    "        ';', \n",
    "        ',')\n",
    ")\n",
    "sdf_ds = sdf_ds.withColumn(\n",
    "    'annotation', \n",
    "    F.regexp_replace(\n",
    "        'annotation', \n",
    "        ';', \n",
    "        ',')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds.coalesce(1).write.csv('articles.csv', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing: AI jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = '../topic_2/ai_jobs_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(files_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(files_path)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mask = f'{files_path}/*.json'\n",
    "sdf = spark.read.json(files_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Understading the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once againg to unpack all inner structures\n",
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: split `salary range` column simply by space\n",
    "split_col = F.split(sdf['salary_range'], ' ')\n",
    "\n",
    "# step 2: extract values from string into columns\n",
    "sdf = sdf.withColumn('currency', split_col.getItem(0))\n",
    "sdf = sdf.withColumn('salary_value_min', split_col.getItem(1))\n",
    "sdf = sdf.withColumn('salary_value_max', split_col.getItem(3))\n",
    "\n",
    "# step 3: convert to numbers\n",
    "sdf = sdf.withColumn('salary_value_min', \n",
    "                     F.regexp_replace('salary_value_min', 'K', ''))\n",
    "sdf = sdf.withColumn('salary_value_min', col('salary_value_min') * 1000)\n",
    "sdf = sdf.withColumn('salary_value_max', \n",
    "                     F.regexp_replace('salary_value_max', 'K', ''))\n",
    "sdf = sdf.withColumn('salary_value_max', col('salary_value_max') * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\n",
    "    'benefits_str',\n",
    "    F.array_join(F.col('benefits'), ',')\n",
    ")\n",
    "sdf = sdf.withColumn(\n",
    "    'skills_str',\n",
    "    F.array_join(F.col('skills'), ',')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds = sdf.select(\n",
    "    # these are target variables\n",
    "    sdf.currency.alias('cur_1'),\n",
    "    sdf.salary_value_min.alias('salary_min_1'),\n",
    "    sdf.salary_value_max.alias('salary_max_1'),\n",
    "    sdf.baseSalary_currency.alias('cur_2'),\n",
    "    sdf.baseSalary_value_minValue.alias('salary_min_2'),\n",
    "    sdf.baseSalary_value_maxValue.alias('salary_max_2'),\n",
    "    sdf.baseSalary_value_unitText.alias('salary_period'),\n",
    "    # that will be predictors\n",
    "    sdf.company,\n",
    "    sdf['hiringOrganization_@type'].alias('company_type'),\n",
    "    sdf.jobLocation_address_addressCountry.alias('country'),\n",
    "    sdf.location,\n",
    "    sdf.position,\n",
    "    sdf.level,\n",
    "    sdf.employmentType.alias('type'),\n",
    "    sdf.benefits_str.alias('benefits'),\n",
    "    sdf.skills_str.alias('skills'),\n",
    "    sdf.description\n",
    ").dropDuplicates()\n",
    "sdf_ds.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds.coalesce(1).write.csv('ai_jobs.csv', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
