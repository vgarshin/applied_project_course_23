{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION TECHNIQUES\n",
    "## Part V. Load from object storage and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries and credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[About boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "creds = access_data(file_path='access_bucket.json')\n",
    "print(creds.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Session and client for loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=creds['aws_access_key_id'],\n",
    "    aws_secret_access_key=creds['aws_secret_access_key'],\n",
    "    endpoint_url='https://storage.yandexcloud.net'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VK_DATA_BUCKET = creds['bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [key['Key'] for key in s3.list_objects(Bucket=VK_DATA_BUCKET)['Contents']]\n",
    "print('files in storage:', all_files[:10]) # works only for num of files < 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load data from the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_load = all_files[1]\n",
    "print('file to load:', file_to_load)\n",
    "get_object_response = s3.get_object(\n",
    "    Bucket=VK_DATA_BUCKET, \n",
    "    Key=file_to_load\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_object_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(get_object_response['Body'])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Time for Big Data (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[5]')\n",
    "conf.set('spark.driver.memory', '12G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', creds['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', creds['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', \n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f's3a://{VK_DATA_BUCKET}/' + all_files[3]\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(F.explode(sdf.items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_df(df, prefix=None):\n",
    "    flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\n",
    "    nested_cols = [c[0] for c in df.dtypes if c[1][:6] == 'struct']\n",
    "    flat_df = df.select(\n",
    "        flat_cols + \n",
    "        [F.col(ncol + '.' + col).alias(prefix + col if prefix else ncol + '_' + col ) \n",
    "         for ncol in nested_cols \n",
    "         for col in df.select(ncol + '.*').columns]\n",
    "    )\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = flat_df(sdf, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load ALL files with posts from ALL walls and users\n",
    "file_path = f's3a://{VK_DATA_BUCKET}/gsom_ma-2023-11-20-01-00-05-925736/walls/wall_owner_id_*.json'\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(F.explode(sdf.groups))\n",
    "sdf = flat_df(sdf, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data processing and basic EDA - VK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = [\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_abiturient-2023-11-14-*/wall_owner_id_*.json',\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_ma-2023-11-20-*/wall_owner_id_*.json',\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_spbu-2023-11-15-*//wall_owner_id_*.json'\n",
    "]\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataset v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = sdf.select(F.explode(sdf.groups), sdf.items)\n",
    "sdf1 = flat_df(sdf1, prefix='')\n",
    "sdf1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = sdf1.select(sdf1.col_id, sdf1.col_name, F.explode(sdf.items))\n",
    "sdf1 = flat_df(sdf1, prefix='')\n",
    "sdf1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.groupBy(\"col_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataset v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_groups = sdf.select(F.explode(sdf.groups))\n",
    "sdf_groups = flat_df(sdf_groups, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_groups.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf.select(F.explode(sdf.items))\n",
    "sdf_posts = flat_df(sdf_posts, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf_posts.withColumn('col_owner_id', F.regexp_replace('col_owner_id', '-', ''))\n",
    "sdf_posts = sdf_posts.withColumn('col_from_id', F.regexp_replace('col_from_id', '-', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf_posts.join(sdf_groups, sdf_posts.col_owner_id == sdf_groups.col_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.groupBy(\"col_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.select('col_name').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.filter(sdf_posts.col_name == 'Высшая школа менеджмента СПбГУ').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts \\\n",
    "    .filter(sdf_posts.col_name == 'Высшая школа менеджмента СПбГУ') \\\n",
    "    .limit(5) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf_posts.select(\n",
    "    sdf_posts.col_date,\n",
    "    sdf_posts.col_comments,\n",
    "    sdf_posts.col_likes,\n",
    "    sdf_posts.col_reposts,\n",
    "    sdf_posts.col_views,\n",
    "    sdf_posts.col_text,\n",
    "    sdf_posts.col_name,\n",
    "    sdf_posts.col_screen_name\n",
    ").dropDuplicates()\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupBy(\"col_name\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data processing and basic EDA - AI Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './ai_jobs_data/*.json'\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.employmentType.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.baseSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.baseSalary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.What is the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once againg to unpack all inner structures\n",
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = F.split(sdf['salary_range'], ' ')\n",
    "\n",
    "sdf = sdf.withColumn('currency', split_col.getItem(0))\n",
    "sdf = sdf.withColumn('salary_value_min', split_col.getItem(1))\n",
    "sdf = sdf.withColumn('salary_value_max', split_col.getItem(3))\n",
    "\n",
    "sdf = sdf.withColumn('salary_value_min', \n",
    "                     F.regexp_replace('salary_value_min', 'K', ''))\n",
    "sdf = sdf.withColumn('salary_value_min', col('salary_value_min') * 1000)\n",
    "sdf = sdf.withColumn('salary_value_max', \n",
    "                     F.regexp_replace('salary_value_max', 'K', ''))\n",
    "sdf = sdf.withColumn('salary_value_max', col('salary_value_max') * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Dataset for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds = sdf.select(\n",
    "    sdf.currency.alias('cur_1'),\n",
    "    sdf.salary_value_min.alias('salary_min_1'),\n",
    "    sdf.salary_value_max.alias('salary_max_1'),\n",
    "    sdf.baseSalary_currency.alias('cur_2'),\n",
    "    sdf.baseSalary_value_maxValue.alias('salary_min_2'),\n",
    "    sdf.baseSalary_value_minValue.alias('salary_max_2'),\n",
    "    sdf.baseSalary_value_unitText.alias('salary_period'),\n",
    "    sdf.benefits.alias('benefits'),\n",
    "    sdf.company,\n",
    "    sdf['hiringOrganization_@type'].alias('company_type'),\n",
    "    sdf.description,\n",
    "    sdf.employmentType.alias('type'),\n",
    "    sdf.level,\n",
    "    sdf.jobLocation_address_addressCountry.alias('country'),\n",
    "    sdf.location,\n",
    "    sdf.position,\n",
    "    sdf.skills,\n",
    ").dropDuplicates()\n",
    "sdf_ds.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf_ds.toPandas()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['float64', 'object']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "sns.violinplot(data=df, x='salary_min_1', y='type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cur_1.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.type.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "df.country.hist(bins=60)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Texts processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('articles_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './articles_data/*.json'\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('file', F.input_file_name())\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\n",
    "    'file', \n",
    "    F.regexp_replace(\n",
    "        'file', \n",
    "        'file:///home/jovyan/apbdid_23/topic_2/articles_data/articles_lbl_', \n",
    "        '')\n",
    ")\n",
    "sdf = sdf.withColumn(\n",
    "    'label', \n",
    "    F.regexp_replace(\n",
    "        'file', \n",
    "        '.json', \n",
    "        '')\n",
    ")\n",
    "sdf.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    sdf.label,\n",
    "    F.explode(sdf.articles)\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds = sdf.select(\n",
    "    sdf.label,\n",
    "    sdf.col_name.alias('title'),\n",
    "    sdf.col_annotation.alias('annotation')\n",
    ")\n",
    "sdf_ds.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_ds.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
